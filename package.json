{
  "name": "htmx-ai-streamer",
  "version": "1.0.0",
  "description": "HTMX interface for streaming LLM calls from llama.cpp",
  "main": "dist/server.js",
  "type": "module",
  "scripts": {
    "dev": "tsx watch src/server.ts",
    "build": "tsc",
    "start": "node dist/server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "node-fetch": "^3.3.2",
    "selfsigned": "^3.0.1"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/node": "^20.11.5",
    "@types/selfsigned": "^2.0.4",
    "tsx": "^4.7.0",
    "typescript": "^5.3.3"
  }
}
